data:
  data_module:
    datasets:
    - spice-des-monomers
    - spice-pubchem
    - gen2
    - gen2-torsion
    - rna-diverse
    - pepconf-dlc
    - protein-torsion
    - spice-dipeptide
    - dipeptides-300K-openff-1.2.0
    - dipeptides-300K-amber99
    - dipeptides-300K-charmm36_nonb
    - dipeptides-1000K-openff-1.2.0
    - dipeptides-1000K-amber99
    - dipeptides-1000K-charmm36_nonb
    - uncapped-300K-openff-1.2.0
    - uncapped-300K-amber99
    - dipeptides-hyp-dop-300K-amber99
    - dipeptides-radical-300K
    - bondbreak-radical-peptides-300K
    pure_test_datasets:
    - rna-trinucleotide
    pure_val_datasets: []
    pure_train_datasets:
    - rna-nucleoside
    splitpath: espaloma_split
    partition:
    - 0.8
    - 0.1
    - 0.1
    train_batch_size: 32
    val_batch_size: 32
    ref_terms:
    - nonbonded
    train_loader_workers: 4
    val_loader_workers: 4
    test_loader_workers: 4
    conf_strategy: 32
    ff_lookup: {}
    seed: 0
    pin_memory: true
    tr_subsampling_factor: null
    weights:
      rna-diverse: 3
      rna-nucleoside: 5
      spice-pubchem: 0.8
    balance_factor: 0.0
    val_conf_strategy: 200
  energy:
    terms:
    - bond
    - angle
    - proper
    - improper
    gradients: true
experiment:
  ckpt_path: /local/user/seutelf/grappa/ckpt/grappa-1.3/baseline/2024-06-19_04-54-30/epoch=784-early_stopping_loss=1.955e+01.ckpt
  ckpt_cfg_override: false
  wandb:
    name: published
    project: grappa-1.3
    save_code: true
    tags: []
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    monitor: early_stopping_loss
    mode: min
    save_top_k: 2
    save_last: true
    every_n_epochs: 5
    auto_insert_metric_name: false
    filename: epoch:{epoch}-early_stop_loss:{early_stopping_loss:.2f}
  progress_bar: true
  trainer:
    overfit_batches: 0
    min_epochs: 5
    max_epochs: 2000
    accelerator: gpu
    log_every_n_steps: 10
    deterministic: false
    check_val_every_n_epoch: 1
  evaluation:
    n_bootstrap: 1000
  train:
    lr: 1.5e-05
    energy_weight: 1.0
    gradient_weight: 0.8
    start_qm_epochs: 2
    param_loss_epochs: 100
    warmup_steps: 500
    early_stopping_energy_weight: 3.0
    param_weight: 0.001
    proper_regularisation: 0.001
    improper_regularisation: 0.001
    weight_decay: 0
    patience: 50
    lr_decay: 0.8
    tuplewise_weight: 0.0
    time_limit: 47.5
model:
  graph_node_features: 256
  in_feats: null
  in_feat_name:
  - atomic_number
  - ring_encoding
  - partial_charge
  - degree
  in_feat_dims: {}
  gnn_width: 512
  gnn_attentional_layers: 7
  gnn_convolutions: 0
  gnn_attention_heads: 16
  gnn_dropout_attention: 0.3
  gnn_dropout_initial: 0.0
  gnn_dropout_conv: 0.0
  gnn_dropout_final: 0.1
  symmetric_transformer_dropout: 0.5
  symmetric_transformer_depth: 1
  symmetric_transformer_n_heads: 8
  symmetric_transformer_width: 512
  symmetriser_depth: 4
  symmetriser_width: 256
  n_periodicity_proper: 3
  n_periodicity_improper: 2
  gated_torsion: true
  positional_encoding: true
  layer_norm: true
  self_interaction: true
  learnable_statistics: false
  torsion_cutoff: 0.0001
  harmonic_gate: false
  only_n2_improper: true
  stat_scaling: true
  shifted_elu: true
