import torch
import dgl


class AttentionLayer(torch.nn.Module):
    def __init__(self) -> None:
        super().__init__()


        